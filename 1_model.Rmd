# CFA {#cfa}

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE
)
```


```{r, include=FALSE, warning=FALSE, message=FALSE}
library(data.table)
library(glue)
library(lavaan)
library(googlesheets4)
library(kableExtra)

# questions <- fread('./data/questions.csv')
# c2_data <- fread('./data/c2_data.csv')
# questionnaire_names <- fread('./data/questionnaire_names.csv')
# 
# 
# c2_info <- questionnaire_names[question_type == 'competencies']
# c2_questions <- c2_info[order(q_order), question_ru]
# c2_q_numbers <- c2_info[order(q_order), q]
# 
# c2_names <- questions[question_ru %in% c2_info[, unique(question_ru)], sort(unique(c2))]
# 
# fit_mlr <- readRDS('fit_mlr')
# fit_mlr_cleaned <- readRDS('fit_mlr_cleaned')
```


## основная модель

Самая простая модель на 8 компетенций среднего уровня (с2). Это исходная модель, которая описана в статье. С помощью конфирматорного факторного анализа проверяем, насколько теоретическая структура модели присутствует в данных.

Декларация модели такая:

```{r}
c2_names <- competencies_info[, unique(c2)]
c2_declarations <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations <- paste(c2_declarations, collapse = '\n\n')

cat(c2_declarations)
```

Модель сходится относительно неплохо:

```{r, include=FALSE, eval=FALSE}
# fit_mlr <- cfa(model = c2_declarations, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr, './cfa_models/fit_mlr')
fit_mlr <- readRDS('./cfa_models/fit_mlr')
```


```{r}
fitMeasures(fit_mlr, c('cfi', 'rmsea', 'srmr'))
```

Индекс CFI (сравнение с моделью без латентных факторов) невысок, так как для хороших моделей значение должно быть выше 0.9, а лучше -- выше 0.95. 
Возможные способы улучшить модель -- пересмотреть факторную структуру или добавить кросс-нагрузки (когда один вопрос относится к нескольким факторам).

Референсные значения для RMSEA (оценка сложности модели) -- чем ниже, тем лучше, ниже 0.05 очень хорошее качество, ниже 0.08 -- приемлемое.

Референсные значения для SRMR (насколько хорошо модель воспроизводит наблюдаемые корреляции) -- чем ниже, тем лучше, ниже 0.05 очень хорошее качество, ниже 0.08 -- приемлемое.


## подходы к улучшению модели

### скорректированная модель 

Самый простой способ улучшить модель -- удалить те вопросы, по которым нет значимых вкладов. В табличке ниже дано количество вопросов в каждой шкале всего, и сколько оказалось без значимых вкладов (кандидаты на удаление):

```{r}
coeffs <- data.table(parameterEstimates(fit_mlr))[op == '=~']
coeffs_lowp <- coeffs[pvalue < 0.05 | is.na(pvalue)]

c2_declarations_cleaned <- coeffs_lowp[, list(paste(lhs, '=~', paste(rhs, collapse = ' + ') )), by = lhs]
c2_declarations_cleaned <- c2_declarations_cleaned[, paste(V1, collapse = '\n\n')]

# fit_mlr_cleaned <- cfa(model = c2_declarations_cleaned, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_cleaned, './cfa_models/fit_mlr_cleaned')
fit_mlr_cleaned <- readRDS('./cfa_models/fit_mlr_cleaned')
```

<!-- Ряд вопросов имеют низкий / незначимый вклад, поэтому их лучше удалить из модели. Вот список компетенций с исходным количеством вопросов и удаленными вопросами: -->

```{r}
coeffs_cleaned <- data.table(parameterEstimates(fit_mlr_cleaned))[op == '=~']
coeffs_cleaned_lowp <- coeffs_cleaned[pvalue < 0.05 | is.na(pvalue)]

scales_questions <- merge(
  # coeffs[, list(full_model = .N), keyby = lhs],
  competencies_info[, list(full_model = .N), keyby = list(lhs = paste0('c', c2), C1, C2)],
  coeffs_cleaned_lowp[, list(trimmed_model = .N), by = lhs],
  by = 'lhs', all.x = TRUE
)
scales_questions[, excluded := full_model - trimmed_model]
setnames(scales_questions, 'lhs', 'c2')

kableExtra::kable(rbind(scales_questions[3:8], scales_questions[1:2])) %>%
  kable_styling(font_size = 12)
```

После удаления вопросов метрики модели выглядят следующим образом:

```{r}
fitMeasures(fit_mlr_cleaned, c('cfi', 'rmsea', 'srmr'))
```


### модель с кросс-нагрузками

```{r}
modindices_high_loads <- modindices_cut[, .SD[1], by = rhs]
modindices_high_loads

coeffs_lowp[, unique(lhs)]

c2_declarations_mod <- lapply(coeffs_lowp[, sort(unique(lhs))], function(x) {
  questions <- c(coeffs_lowp[lhs == x, rhs], modindices_high_loads[lhs == x, rhs])
  questions <- sort(unique(questions))
  model <- paste(questions, collapse = ' + ')
  paste(x, '=~', model)
})

# c2_declarations_mod <- lapply(coeffs_lowp[, unique(lhs)], function(x) {
#   if (x == 'c3') {
#     questions <- c(coeffs_lowp[lhs %in% c('c3', 'c9'), rhs], modindices_high_loads[lhs %in% c('c3', 'c9'), rhs])
#   }
#   else {
#     questions <- c(coeffs_lowp[lhs == x, rhs], modindices_high_loads[lhs == x, rhs])
#   }
#   questions <- sort(unique(questions))
#   model <- paste(questions, collapse = ' + ')
#   paste(x, '=~', model)
# })
# c2_declarations_mod <- c2_declarations_mod[-6]

c2_declarations_mod <- paste(c2_declarations_mod, collapse = '\n')
cat(c2_declarations_mod)
```

```{r}
# fit_mlr_mod <- cfa(model = c2_declarations_cleaned, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_mod, './cfa_models/fit_mlr_mod')

fit_mlr_mod <- readRDS('./cfa_models/fit_mlr_mod')
```

```{r}
fitMeasures(fit_mlr_mod, c('cfi', 'rmsea', 'srmr'))
```


### модель без кросс-нагрузок

Еще один способ улучшить модель --- исключить вопросы с высокой кросс-нагрузкой (вариативность которых может объясняться сразу несколькими латентными факторами).

Можно, наоборот, попробовать сделать модель с учетом кросс-нагрузок и указать связь вопросов сразу с несколькими факторами. Однако достаточно сложно определить хорошие кандидаты на связь с другими факторам, так как некоторые вопросы имеют высокую кросс-нагрузку сразу по нескольким факторам. 

```{r}
modindices_cut <- modindices[op == '=~' & mi > 10]
modindices_cut <- modindices_cut[order(rhs, -mi)]
modindices_cut[, mi_rnd := round(mi, 2)]
```


```{r}
# смотрим кросс-нагрузки
# modindices <- modindices(fit_mlr_cleaned, sort = TRUE)
# saveRDS(modindices, './cfa_models/modindices_cleaned')
modindices <- readRDS('./cfa_models/modindices_cleaned')
setDT(modindices)
```

```{r}
# вопросы с высокими кросс-нагрузками
modindices_cut <- modindices[op == '=~' & mi > 10]
modindices_cut <- modindices_cut[order(rhs, -mi)]
modindices_cut[, mi_rnd := round(mi, 2)]

indices_table <- rbind(
  modindices_cut[, list(type = 'cross-loads', lhs, rhs, mi = round(mi, 2))],
  coeffs_lowp[, list(type = 'model', lhs, rhs, mi = 100, pvalue = round(pvalue, 5))],
  fill = TRUE
)

indices_table[, n_factors := .N, by = rhs]
indices_table <- indices_table[n_factors > 1]

indices_table <- merge(
  indices_table,
  questionnaire_names[, list(rhs = q, question_ru, c3)],
  by = 'rhs', all.x = TRUE
)

indices_table <- merge(
  indices_table,
  unique(big_model[, list(lhs = paste0('c', c2), C1, C2)]),
  by = c('lhs'), all.x = TRUE
)

indices_table <- merge(
  indices_table,
  unique(big_model[, list(lhs = paste0('c', c2), c3, C3, description)]),
  by = c('lhs', 'c3'), all.x = TRUE
)

indices_table <- indices_table[order(rhs, -type, -mi)]
indices_table[, n_factors := NULL]

write_sheet(
  data = indices_table,
  ss = ss,
  sheet = 'cross-loads'
)

```

```{r}
# выделяем вопросы, в которых mi > N (индекс кросс-нагрузок)
questions_highloads <- modindices[op == '=~' & mi > 15, unique(rhs)]
length(questions_highloads)

# c2_declarations_cleaned_noloads <- gsub(paste(questions_highloads, collapse = '|'), '', c2_declarations_cleaned)
# cat(c2_declarations_cleaned_noloads)

# задаем модель
c2_declarations_cleaned_noloads <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% questions_highloads, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_noloads <- paste(c2_declarations_cleaned_noloads, collapse = '\n\n')
cat(c2_declarations_cleaned_noloads)
```

```{r}
# fit_mlr_noloads_20 <- cfa(model = c2_declarations_cleaned_noloads, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_noloads_20, './cfa_models/fit_mlr_noloads_20')
fit_mlr_noloads_20 <- readRDS('./cfa_models/fit_mlr_noloads_20')
```

```{r}
# fit_mlr_noloads_15 <- cfa(model = c2_declarations_cleaned_noloads, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_noloads_15, './cfa_models/fit_mlr_noloads_15')
fit_mlr_noloads_15 <- readRDS('./cfa_models/fit_mlr_noloads_15')
```


Метрики качество модели при удалении вопросов с высокими (mi > 15) кросс-нагрузками
```{r}
fitMeasures(fit_mlr_noloads_15, c('cfi', 'rmsea', 'srmr'))
# fitMeasures(fit_mlr_noloads_20, c('cfi', 'rmsea', 'srmr'))
```

### модель с высокими efa-нагрузками

Третий способ, который я использовал --- убрал из модели вопросы, которые имели низкую факторную нагрузку при разбиении вопросов на восемь факторов при EFA. По сути, я убрал совсем нерелевантные вопросы.


```{r}
efa_result <- fa(competencies_data, nfactors = 8, rotate = "oblimin", fm = "ml")
loadings_dt <- as.data.table(unclass(efa_result$loadings), keep.rownames = 'q')
loadings_dt[, max_loading := do.call(pmax, lapply(.SD, abs)), .SDcols = patterns('ML')]
# loadings_dt[max_loading < 0.3, q]
```


```{r}
# выделяем вопросы, в которых mi > N (индекс кросс-нагрузок)
questions_weak <- loadings_dt[max_loading < 0.3, unique(q)]
length(questions_weak)

# c2_declarations_cleaned_noloads <- gsub(paste(questions_highloads, collapse = '|'), '', c2_declarations_cleaned)
# cat(c2_declarations_cleaned_noloads)

# задаем модель
c2_declarations_cleaned_weak <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% questions_weak, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_weak <- paste(c2_declarations_cleaned_weak, collapse = '\n\n')
cat(c2_declarations_cleaned_weak)
```
```{r}
# fit_mlr_weak <- cfa(model = c2_declarations_cleaned_weak, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_weak, './cfa_models/fit_mlr_weak')
fit_mlr_weak <- readRDS('./cfa_models/fit_mlr_weak')
```


Метрики качества получившейся модели:
```{r}
fitMeasures(fit_mlr_weak, c('cfi', 'rmsea', 'srmr'))
```

### модель без вопросов с низкой нагрузкой

Четвертый способ (самый слабый из испробованных) -- удаление нерелевантных вопросов из шкал `отношения-1`, `работа с изменениями`, `профессионализм-2`, `результаты`.

```{r}
# задаем модель
c2_declarations_cleaned_weak2 <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% q_excluded, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_weak2 <- paste(c2_declarations_cleaned_weak2, collapse = '\n\n')
cat(c2_declarations_cleaned_weak2)
```

```{r}
# fit_mlr_weak2 <- cfa(model = c2_declarations_cleaned_weak2, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_weak2, './cfa_models/fit_mlr_weak2')
fit_mlr_weak2 <- readRDS('./cfa_models/fit_mlr_weak2')
```

```{r}
fitMeasures(fit_mlr_weak, c('cfi', 'rmsea', 'srmr'))
```


## метрики с2-моделей

Сводная таблица метрик качества этих моделей выглядит следующим образом:

```{r}
c2_models_metrics <- data.table(
  model = c(
    'исходная модель', 
    'модель без незначимых вопросов', 
    'модель с кросс-нагрузками', 
    'модель без кросс-нагрузок',
    'модель с высокими EFA-нагрузками',
    'модель без низких EFA-нагрузок по отдельным шкалам')
)
```

```{r}
metrics_names <- c('cfi', 'rmsea', 'srmr')
c2_models_metrics[model == 'исходная модель', (metrics_names) := as.list(round(fitMeasures(fit_mlr, metrics_names), 3))]
c2_models_metrics[model == 'модель без незначимых вопросов', (metrics_names) := as.list(round(fitMeasures(fit_mlr_cleaned, metrics_names), 3))]
c2_models_metrics[model == 'модель с кросс-нагрузками', (metrics_names) := as.list(round(fitMeasures(fit_mlr_mod, metrics_names), 3))]
c2_models_metrics[model == 'модель без кросс-нагрузок', (metrics_names) := as.list(round(fitMeasures(fit_mlr_noloads_15, metrics_names), 3))]
c2_models_metrics[model == 'модель с высокими EFA-нагрузками', (metrics_names) := as.list(round(fitMeasures(fit_mlr_weak, metrics_names), 3))]
c2_models_metrics[model == 'модель без низких EFA-нагрузок по отдельным шкалам', (metrics_names) := as.list(round(fitMeasures(fit_mlr_weak2, metrics_names), 3))]

kableExtra::kable(c2_models_metrics)  %>%
  kable_styling(font_size = 12)
```


Удаление незначимых вопросов немного улучшает качество структуры модели, но оно все еще недостаточно. Остальные попытки улучшить качество модели не дали хороших эффектов, даже наоборот. 

Дальнейшие улучшения текущей можели на восемь компетенций мне кажутся бессмысленными. Тем не менее, можно попробовать оценить качество модели по 30 субшкалам (то есть по субшкалам наших восьми компетенций), что я и сделал ниже. Либо в целом попробовать пересобрать факторную модель в эксплораторном факторном анализе (в следующем разделе).


## модель с3-шкал

В качестве альтернативного решения я попробовал оценить структуру опросника на уровне дробных с3-шкал. Может быть, осмысленно спуститься на уровень ниже, от восьми шкал компетенций на уровень из субшкал (исходно в модели их 42, в опроснике представлено 30)

Однако модель на 30 факторов не сходится, даже если предположить скоррелированность латентных факторов.

```{r}
c3_names <- competencies_info[, sort(unique(c3))]

c3_declarations <- lapply(c3_names, function(x) competencies_info[c3 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c3_declarations <- paste(c3_declarations, collapse = '\n\n')
# cat(c3_declarations)
```


```{r}
# fit_c3_mlr <- cfa(model = c3_declarations, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_c3_mlr, './cfa_models/fit_c3_mlr')
fit_c3_mlr <- readRDS('./cfa_models/fit_c3_mlr')
```


Если рассматривать каждую из 30 шкал отдельно, то видно, что есть несколько шкал, по которым даже маленькая модель сойтись не может. 

```{r, include=FALSE}
# c3_models <- lapply(c3_names, function(x) {
#   tmp_model <- competencies_info[c3 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))]
#   tmp_model_fit <- cfa(model = tmp_model, data = competencies_data, estimator = 'MLR')
#   return(tmp_model_fit)
# })
# 
# saveRDS(c3_models, './cfa_models/c3_models')
c3_models <- readRDS('./cfa_models/c3_models')
```



```{r}
c3_models_tbl <- lapply(1:length(c3_names), function(x) {
  result <- tryCatch(
      # as.data.table(as.list(c('c3' = c3_names[x], fitMeasures(c3_models[[x]], c("cfi", "rmsea", "srmr", "chisq", "df"))))),
      as.data.table(as.list(c('c3' = c3_names[x], round(fitMeasures(c3_models[[x]], c("cfi", "rmsea", "srmr")), 3)))),
    error = function(e) data.table(c3 = c3_names[x], cfi = NA, rmsea = NA, srmr = NA)
  )
  # print(result)
  result
})
c3_models_tbl <- rbindlist(c3_models_tbl)
c3_models_tbl <- merge(
  competencies_info[, list(c2 = unique(c2), n_questions = .N), keyby = c3], 
  c3_models_tbl, 
  by = 'c3', all.x = TRUE
)
c3_models_tbl[, cfi := cell_spec(cfi, color = ifelse(cfi >= 0.9 | is.na(cfi), "black", "red"))]
c3_models_tbl[, rmsea := cell_spec(rmsea, color = ifelse(rmsea < 0.08 | is.na(rmsea), "black", "red"))]
c3_models_tbl[, srmr := cell_spec(srmr, color = ifelse(srmr < 0.08 | is.na(srmr), "black", "red"))]

kableExtra::kable(c3_models_tbl) %>%
  kable_styling(font_size = 12)
```

Как правило, такое происходит в нескольких ситуациях:

 - вопросы либо очень слабо, либо очень сильно скоррелированы
 - есть много пропусков или недостаточно респондентов
 - не учтена ситуация, когда один вопрос может в реальности относиться к нескольким факторам (кросс-нагрузки)



