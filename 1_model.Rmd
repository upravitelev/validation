# CFA {#cfa}

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE
)
```


```{r, include=FALSE, warning=FALSE, message=FALSE}
library(data.table)
library(glue)
library(lavaan)
library(googlesheets4)
library(kableExtra)

# questions <- fread('./data/questions.csv')
# c2_data <- fread('./data/c2_data.csv')
# questionnaire_names <- fread('./data/questionnaire_names.csv')
# 
# 
# c2_info <- questionnaire_names[question_type == 'competencies']
# c2_questions <- c2_info[order(q_order), question_ru]
# c2_q_numbers <- c2_info[order(q_order), q]
# 
# c2_names <- questions[question_ru %in% c2_info[, unique(question_ru)], sort(unique(c2))]
# 
# fit_mlr <- readRDS('fit_mlr')
# fit_mlr_cleaned <- readRDS('fit_mlr_cleaned')
```


## основная модель

Самая простая модель на 8 компетенций среднего уровня (с2). Это исходная модель, которая описана в статье. С помощью конфирматорного факторного анализа проверяем, насколько теоретическая структура модели присутствует в данных.

Декларация модели такая:

```{r}
c2_names <- competencies_info[, unique(c2)]
c2_declarations <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations <- paste(c2_declarations, collapse = '\n\n')

cat(c2_declarations)
```

Модель сходится относительно неплохо:

```{r, include=FALSE, eval=FALSE}
# fit_mlr <- cfa(model = c2_declarations, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr, './cfa_models/fit_mlr')
fit_mlr <- readRDS('./cfa_models/fit_mlr')
```


```{r}
fitMeasures(fit_mlr, c('cfi', 'rmsea', 'srmr'))
```

Индекс CFI (сравнение с моделью без латентных факторов) невысок, так как для хороших моделей значение должно быть выше 0.9, а лучше -- выше 0.95. 
Возможные способы улучшить модель -- пересмотреть факторную структуру или добавить кросс-нагрузки (когда один вопрос относится к нескольким факторам).

Референсные значения для RMSEA (оценка сложности модели) -- чем ниже, тем лучше, ниже 0.05 очень хорошее качество, ниже 0.08 -- приемлемое.

Референсные значения для SRMR (насколько хорошо модель воспроизводит наблюдаемые корреляции) -- чем ниже, тем лучше, ниже 0.05 очень хорошее качество, ниже 0.08 -- приемлемое.


## скорректированная модель 

```{r}
coeffs <- data.table(parameterEstimates(fit_mlr))[op == '=~']
coeffs_lowp <- coeffs[pvalue < 0.05 | is.na(pvalue)]

c2_declarations_cleaned <- coeffs_lowp[, list(paste(lhs, '=~', paste(rhs, collapse = ' + ') )), by = lhs]
c2_declarations_cleaned <- c2_declarations_cleaned[, paste(V1, collapse = '\n\n')]

# fit_mlr_cleaned <- cfa(model = c2_declarations_cleaned, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_cleaned, './cfa_models/fit_mlr_cleaned')
fit_mlr_cleaned <- readRDS('./cfa_models/fit_mlr_cleaned')
```


Ряд вопросов имеют низкий / незначимый вклад, поэтому их лучше удалить из модели. Вот список компетенций с исходным количеством вопросов и удаленными вопросами:

```{r}
coeffs_cleaned <- data.table(parameterEstimates(fit_mlr_cleaned))[op == '=~']
coeffs_cleaned_lowp <- coeffs_cleaned[pvalue < 0.05 | is.na(pvalue)]

scales_questions <- merge(
  # coeffs[, list(full_model = .N), keyby = lhs],
  competencies_info[, list(full_model = .N), keyby = list(lhs = paste0('c', c2), C1, C2)],
  coeffs_cleaned_lowp[, list(trimmed_model = .N), by = lhs],
  by = 'lhs', all.x = TRUE
)
scales_questions[, excluded := full_model - trimmed_model]
setnames(scales_questions, 'lhs', 'c2')

kableExtra::kable(rbind(scales_questions[3:8], scales_questions[1:2])) %>%
  kable_styling(font_size = 12)
```

После удаления вопросов метрики модели выглядят следующим образом:

```{r}
fitMeasures(fit_mlr_cleaned, c('cfi', 'rmsea', 'srmr'))
```


## метрики качества c2-моделей

Метрики качества модели без знасимых вопросов практически не изменились. Сводная таблица всех четырех моделей (с корреляцией факторов / без корреляции, с удалением незначимых вопровов / все вопросы) выглядит следующим образом:

```{r}
rbind(
  as.data.table(as.list(c('model' = 'all questions', round(fitMeasures(fit_mlr, c('cfi', 'rmsea', 'srmr')), 3)))),
  as.data.table(as.list(c('model' = 'excluded questions', round(fitMeasures(fit_mlr_cleaned, c('cfi', 'rmsea', 'srmr')), 3))))
)
```

Удаление незначимых вопросов немного улучшает качество структуры модели, но оно все еще недостаточно. Дальнейшие действия возможные тут --- добавление кросс-нагрузок вопросов в разные факторы. Либо пересмотр всей модели в целом с помощью разведочного факторного анализа.

Однако это все требует теоретического пересмотра модели компетенций, и может быть излишним для текущей задачи.

## модель без кросс-нагрузок

```{r}
# смотрим кросс-нагрузки
# modindices <- modindices(fit_mlr_cleaned, sort = TRUE)
# saveRDS(modindices, './cfa_models/modindices_cleaned')
modindices <- readRDS('./cfa_models/modindices_cleaned')
setDT(modindices)
```

```{r}
# вопросы с высокими кросс-нагрузками
modindices_cut <- modindices[op == '=~' & mi > 10]
modindices_cut <- modindices_cut[order(rhs, -mi)]
modindices_cut[, mi_rnd := round(mi, 2)]
```

```{r}
# выделяем вопросы, в которых mi > N (индекс кросс-нагрузок)
questions_highloads <- modindices[op == '=~' & mi > 15, unique(rhs)]
length(questions_highloads)

# c2_declarations_cleaned_noloads <- gsub(paste(questions_highloads, collapse = '|'), '', c2_declarations_cleaned)
# cat(c2_declarations_cleaned_noloads)

# задаем модель
c2_declarations_cleaned_noloads <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% questions_highloads, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_noloads <- paste(c2_declarations_cleaned_noloads, collapse = '\n\n')
cat(c2_declarations_cleaned_noloads)
```

```{r}
# fit_mlr_noloads_20 <- cfa(model = c2_declarations_cleaned_noloads, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_noloads_20, './cfa_models/fit_mlr_noloads_20')
fit_mlr_noloads_20 <- readRDS('./cfa_models/fit_mlr_noloads_20')
```

```{r}
# fit_mlr_noloads_15 <- cfa(model = c2_declarations_cleaned_noloads, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_noloads_15, './cfa_models/fit_mlr_noloads_15')
fit_mlr_noloads_15 <- readRDS('./cfa_models/fit_mlr_noloads_15')
```



```{r}
fitMeasures(fit_mlr_noloads_15, c('cfi', 'rmsea', 'srmr'))
fitMeasures(fit_mlr_noloads_20, c('cfi', 'rmsea', 'srmr'))
```

## модель с высокими нагрузками

```{r}
efa_result <- fa(competencies_data, nfactors = 8, rotate = "oblimin", fm = "ml")
loadings_dt <- as.data.table(unclass(efa_result$loadings), keep.rownames = 'q')
loadings_dt[, max_loading := do.call(pmax, lapply(.SD, abs)), .SDcols = patterns('ML')]
loadings_dt[max_loading < 0.3, q]
```


```{r}
# выделяем вопросы, в которых mi > N (индекс кросс-нагрузок)
questions_weak <- loadings_dt[max_loading < 0.3, unique(q)]
length(questions_weak)

# c2_declarations_cleaned_noloads <- gsub(paste(questions_highloads, collapse = '|'), '', c2_declarations_cleaned)
# cat(c2_declarations_cleaned_noloads)

# задаем модель
c2_declarations_cleaned_weak <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% questions_weak, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_weak <- paste(c2_declarations_cleaned_weak, collapse = '\n\n')
cat(c2_declarations_cleaned_weak)
```
```{r}
# fit_mlr_weak <- cfa(model = c2_declarations_cleaned_weak, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_weak, './cfa_models/fit_mlr_weak')
fit_mlr_weak <- readRDS('./cfa_models/fit_mlr_weak')
```

```{r}
fitMeasures(fit_mlr_weak, c('cfi', 'rmsea', 'srmr'))
```

## модель с без вопросов с низкой нагрузкой

```{r}
# задаем модель
c2_declarations_cleaned_weak2 <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% q_excluded, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_weak2 <- paste(c2_declarations_cleaned_weak2, collapse = '\n\n')
cat(c2_declarations_cleaned_weak2)
```

```{r}
# fit_mlr_weak2 <- cfa(model = c2_declarations_cleaned_weak2, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_weak2, './cfa_models/fit_mlr_weak2')
fit_mlr_weak2 <- readRDS('./cfa_models/fit_mlr_weak2')
```

```{r}
fitMeasures(fit_mlr_weak, c('cfi', 'rmsea', 'srmr'))
```


## модель с3-шкал

В качестве альтернативного решения я попробовал оценить структуру опросника на уровне дробных с3-шкал. Может быть, осмысленно спуститься на уровень ниже, от восьми шкал компетенций на уровень из субшкал (исходно в модели их 42, в опроснике представлено 30)

Однако модель на 30 факторов не сходится, даже если предположить скоррелированность латентных факторов.

```{r}
c3_names <- competencies_info[, sort(unique(c3))]

c3_declarations <- lapply(c3_names, function(x) competencies_info[c3 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c3_declarations <- paste(c3_declarations, collapse = '\n\n')
# cat(c3_declarations)
```


```{r}
# fit_c3_mlr <- cfa(model = c3_declarations, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_c3_mlr, './cfa_models/fit_c3_mlr')
fit_c3_mlr <- readRDS('./cfa_models/fit_c3_mlr')
```



```{r, include=FALSE}
# c3_models <- lapply(c3_names, function(x) {
#   tmp_model <- competencies_info[c3 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))]
#   tmp_model_fit <- cfa(model = tmp_model, data = competencies_data, estimator = 'MLR')
#   return(tmp_model_fit)
# })
# 
# saveRDS(c3_models, './cfa_models/c3_models')
c3_models <- readRDS('./cfa_models/c3_models')
```

Если рассматривать каждую из 30 шкал отдельно, то видно, что есть несколько шкал, по которым даже маленькая модель сойтись не может. 

```{r}
c3_models_tbl <- lapply(1:length(c3_names), function(x) {
  result <- tryCatch(
      # as.data.table(as.list(c('c3' = c3_names[x], fitMeasures(c3_models[[x]], c("cfi", "rmsea", "srmr", "chisq", "df"))))),
      as.data.table(as.list(c('c3' = c3_names[x], round(fitMeasures(c3_models[[x]], c("cfi", "rmsea", "srmr")), 3)))),
    error = function(e) data.table(c3 = c3_names[x], cfi = NA, rmsea = NA, srmr = NA)
  )
  # print(result)
  result
})
c3_models_tbl <- rbindlist(c3_models_tbl)
c3_models_tbl <- merge(
  competencies_info[, list(c2 = unique(c2), n_questions = .N), keyby = c3], 
  c3_models_tbl, 
  by = 'c3', all.x = TRUE
)
c3_models_tbl[, cfi := cell_spec(cfi, color = ifelse(cfi >= 0.9 | is.na(cfi), "black", "red"))]
c3_models_tbl[, rmsea := cell_spec(rmsea, color = ifelse(rmsea < 0.08 | is.na(rmsea), "black", "red"))]
c3_models_tbl[, srmr := cell_spec(srmr, color = ifelse(srmr < 0.08 | is.na(srmr), "black", "red"))]

kableExtra::kable(c3_models_tbl) %>%
  kable_styling(font_size = 12)
```

Как правило, такое происходит в нескольких ситуациях:

 - вопросы либо очень слабо, либо очень сильно скоррелированы
 - есть много пропусков или недостаточно респондентов
 - не учтена ситуация, когда один вопрос может в реальности относиться к нескольким факторам (кросс-нагрузки)



