# CFA {#cfa}

## основная модель

Самая простая модель на 8 компетенций среднего уровня (с2). Это исходная модель, которая описана в статье. С помощью конфирматорного факторного анализа проверяем, насколько теоретическая структура модели присутствует в данных.

Декларация модели такая:

```{r}
c2_names <- competencies_info[, unique(c2)]
c2_declarations <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations <- paste(c2_declarations, collapse = '\n\n')

cat(c2_declarations)
```

Модель сходится посредственно:

```{r, include=FALSE, eval=FALSE}
# fit_mlr <- cfa(model = c2_declarations, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr, './cfa_models/fit_mlr')
fit_mlr <- readRDS('./cfa_models/fit_mlr')
```


```{r}
fitMeasures(fit_mlr, c('cfi', 'rmsea', 'srmr'))
```

Индекс CFI (сравнение с моделью без латентных факторов) невысок, так как для хороших моделей значение должно быть выше 0.9, а лучше -- выше 0.95. 
Возможные способы улучшить модель -- пересмотреть факторную структуру или добавить кросс-нагрузки (когда один вопрос относится к нескольким факторам).

Референсные значения для RMSEA (оценка сложности модели) -- чем ниже, тем лучше, ниже 0.05 очень хорошее качество, ниже 0.08 -- приемлемое.

Референсные значения для SRMR (насколько хорошо модель воспроизводит наблюдаемые корреляции) -- чем ниже, тем лучше, ниже 0.05 очень хорошее качество, ниже 0.08 -- приемлемое.


## подходы к улучшению модели

### скорректированная модель 

Самый простой способ улучшить модель -- удалить те вопросы, по которым нет значимых вкладов. В табличке ниже дано количество вопросов в каждой шкале всего, и сколько оказалось без значимых вкладов (кандидаты на удаление):

```{r}
coeffs <- data.table(parameterEstimates(fit_mlr))[op == '=~']
coeffs_lowp <- coeffs[pvalue < 0.05 | is.na(pvalue)]

c2_declarations_cleaned <- coeffs_lowp[, list(paste(lhs, '=~', paste(rhs, collapse = ' + ') )), by = lhs]
c2_declarations_cleaned <- c2_declarations_cleaned[, paste(V1, collapse = '\n\n')]

# fit_mlr_cleaned <- cfa(model = c2_declarations_cleaned, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_cleaned, './cfa_models/fit_mlr_cleaned')
fit_mlr_cleaned <- readRDS('./cfa_models/fit_mlr_cleaned')
```

<!-- Ряд вопросов имеют низкий / незначимый вклад, поэтому их лучше удалить из модели. Вот список компетенций с исходным количеством вопросов и удаленными вопросами: -->

```{r}
coeffs_cleaned <- data.table(parameterEstimates(fit_mlr_cleaned))[op == '=~']
coeffs_cleaned_lowp <- coeffs_cleaned[pvalue < 0.05 | is.na(pvalue)]

scales_questions <- merge(
  # coeffs[, list(full_model = .N), keyby = lhs],
  competencies_info[, list(full_model = .N), keyby = list(lhs = paste0('c', c2), C1, C2)],
  coeffs_cleaned_lowp[, list(trimmed_model = .N), by = lhs],
  by = 'lhs', all.x = TRUE
)
scales_questions[, excluded := full_model - trimmed_model]
setnames(scales_questions, 'lhs', 'c2')

kableExtra::kable(rbind(scales_questions[3:8], scales_questions[1:2])) %>%
  kable_styling(font_size = 12)
```

После удаления вопросов метрики модели выглядят следующим образом:

```{r}
fitMeasures(fit_mlr_cleaned, c('cfi', 'rmsea', 'srmr'))
```


### модель с кросс-нагрузками

И EFA, и метрики кросс-нагрузков CFA показывают, что много вопросов делают вклад в несколько факторов (т. е.  высокая кросс-факторная нагрузка). [Таблица](https://docs.google.com/spreadsheets/d/1kjfYXDjMQ-RXZSFKUem_ABaXFhO1qg6Y4KLeFZD0DeM/edit?gid=1290465890#gid=1290465890), в которой для каждого вопроса даны факторы с максимальной нагрузкой -- по модели и, если есть, кросс-факторы.

```{r}
modindices_cut <- modindices[op == '=~' & mi > 10]
modindices_cut <- modindices_cut[order(rhs, -mi)]
modindices_cut[, mi_rnd := round(mi, 2)]
```


```{r}
# смотрим кросс-нагрузки
# modindices <- modindices(fit_mlr_cleaned, sort = TRUE)
# saveRDS(modindices, './cfa_models/modindices_cleaned')
modindices <- readRDS('./cfa_models/modindices_cleaned')
setDT(modindices)
```

```{r}
# вопросы с высокими кросс-нагрузками
modindices_cut <- modindices[op == '=~' & mi > 10]
modindices_cut <- modindices_cut[order(rhs, -mi)]
modindices_cut[, mi_rnd := round(mi, 2)]

indices_table <- rbind(
  modindices_cut[, list(type = 'cross-loads', lhs, rhs, mi = round(mi, 2))],
  coeffs_lowp[, list(type = 'model', lhs, rhs, mi = 100, pvalue = round(pvalue, 5))],
  fill = TRUE
)

indices_table[, n_factors := .N, by = rhs]
indices_table <- indices_table[n_factors > 1]

indices_table <- merge(
  indices_table,
  competencies_info[, list(rhs = q, question_ru, c3)],
  by = 'rhs', all.x = TRUE
)

indices_table <- merge(
  indices_table,
  unique(big_model[, list(lhs = paste0('c', c2), C1, C2)]),
  by = c('lhs'), all.x = TRUE
)

indices_table <- merge(
  indices_table,
  unique(big_model[, list(lhs = paste0('c', c2), c3, C3, description)]),
  by = c('lhs', 'c3'), all.x = TRUE
)

indices_table <- indices_table[order(rhs, -type, -mi)]
indices_table[, n_factors := NULL]

write_sheet(
  data = indices_table,
  ss = ss,
  sheet = 'cross-loads'
)
```


Соответственно, можно попробовать учесть в модели, что некоторые вопросы могут относиться к нескольким факторам. Для этого я взял первый по кросс-нагрузке фактор для каждого вопроса и добавил к скорректированной модели (основной модели, из которой удалены незначимые вопросы).

Декларация модели:
```{r}
modindices_high_loads <- modindices_cut[, .SD[1], by = rhs]
modindices_high_loads

coeffs_lowp[, unique(lhs)]

c2_declarations_mod <- lapply(coeffs_lowp[, sort(unique(lhs))], function(x) {
  questions <- c(coeffs_lowp[lhs == x, rhs], modindices_high_loads[lhs == x, rhs])
  questions <- sort(unique(questions))
  model <- paste(questions, collapse = ' + ')
  paste(x, '=~', model)
})

# c2_declarations_mod <- lapply(coeffs_lowp[, unique(lhs)], function(x) {
#   if (x == 'c3') {
#     questions <- c(coeffs_lowp[lhs %in% c('c3', 'c9'), rhs], modindices_high_loads[lhs %in% c('c3', 'c9'), rhs])
#   }
#   else {
#     questions <- c(coeffs_lowp[lhs == x, rhs], modindices_high_loads[lhs == x, rhs])
#   }
#   questions <- sort(unique(questions))
#   model <- paste(questions, collapse = ' + ')
#   paste(x, '=~', model)
# })
# c2_declarations_mod <- c2_declarations_mod[-6]

c2_declarations_mod <- paste(c2_declarations_mod, collapse = '\n')
cat(c2_declarations_mod)
```

```{r}
# fit_mlr_mod <- cfa(model = c2_declarations_cleaned, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_mod, './cfa_models/fit_mlr_mod')

fit_mlr_mod <- readRDS('./cfa_models/fit_mlr_mod')
```

В результате получилось такое качество модели:

```{r}
fitMeasures(fit_mlr_mod, c('cfi', 'rmsea', 'srmr'))
```


### модель без кросс-нагрузок

Еще один способ улучшить модель --- исключить вопросы с высокой кросс-нагрузкой (вариативность которых может объясняться сразу несколькими латентными факторами).

Я исключил вопросы, у которых параметр mi > 15. Пробовал также mi > 20, но существенных различий не нашел.

Декларация модели:
```{r}
# выделяем вопросы, в которых mi > N (индекс кросс-нагрузок)
questions_highloads <- modindices[op == '=~' & mi > 15, unique(rhs)]
length(questions_highloads)

# c2_declarations_cleaned_noloads <- gsub(paste(questions_highloads, collapse = '|'), '', c2_declarations_cleaned)
# cat(c2_declarations_cleaned_noloads)

# задаем модель
c2_declarations_cleaned_noloads <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% questions_highloads, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_noloads <- paste(c2_declarations_cleaned_noloads, collapse = '\n\n')
cat(c2_declarations_cleaned_noloads)
```

```{r}
# fit_mlr_noloads_20 <- cfa(model = c2_declarations_cleaned_noloads, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_noloads_20, './cfa_models/fit_mlr_noloads_20')
fit_mlr_noloads_20 <- readRDS('./cfa_models/fit_mlr_noloads_20')
```

```{r}
# fit_mlr_noloads_15 <- cfa(model = c2_declarations_cleaned_noloads, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_noloads_15, './cfa_models/fit_mlr_noloads_15')
fit_mlr_noloads_15 <- readRDS('./cfa_models/fit_mlr_noloads_15')
```


Метрики качества модели при удалении вопросов с высокими (mi > 15) кросс-нагрузками:
```{r}
fitMeasures(fit_mlr_noloads_15, c('cfi', 'rmsea', 'srmr'))
# fitMeasures(fit_mlr_noloads_20, c('cfi', 'rmsea', 'srmr'))
```

### модель с высокими efa-нагрузками

Четвертый способ, который я использовал --- убрал из модели вопросы, которые имели низкую факторную нагрузку (меньше 0.3) при разбиении вопросов на восемь факторов при EFA. По сути, я убрал совсем нерелевантные вопросы.

```{r}
efa_result <- fa(competencies_data, nfactors = 8, rotate = "oblimin", fm = "ml")
loadings_dt <- as.data.table(unclass(efa_result$loadings), keep.rownames = 'q')
loadings_dt[, max_loading := do.call(pmax, lapply(.SD, abs)), .SDcols = patterns('ML')]
# loadings_dt[max_loading < 0.3, q]
```

Декларация модели: 
```{r}
# выделяем вопросы, в которых mi > N (индекс кросс-нагрузок)
questions_weak <- loadings_dt[max_loading < 0.3, unique(q)]
length(questions_weak)

# c2_declarations_cleaned_noloads <- gsub(paste(questions_highloads, collapse = '|'), '', c2_declarations_cleaned)
# cat(c2_declarations_cleaned_noloads)

# задаем модель
c2_declarations_cleaned_weak <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% questions_weak, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_weak <- paste(c2_declarations_cleaned_weak, collapse = '\n\n')
cat(c2_declarations_cleaned_weak)
```

```{r}
# fit_mlr_weak <- cfa(model = c2_declarations_cleaned_weak, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_weak, './cfa_models/fit_mlr_weak')
fit_mlr_weak <- readRDS('./cfa_models/fit_mlr_weak')
```

Метрики качества получившейся модели:
```{r}
fitMeasures(fit_mlr_weak, c('cfi', 'rmsea', 'srmr'))
```

### модель без вопросов с низкой нагрузкой

Пятый способ (самый слабый) -- удаление нерелевантных вопросов из шкал `отношения-1`, `работа с изменениями`, `профессионализм-2`, `результаты` (по результатам оценки факторной структуры этих больших по количеству вопросов шкал).

Декларация модели: 
```{r}
# задаем модель
c2_declarations_cleaned_weak2 <- lapply(c2_names, function(x) competencies_info[c2 == x & question_ru %in% c2_questions & !q %in% q_excluded, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c2_declarations_cleaned_weak2 <- paste(c2_declarations_cleaned_weak2, collapse = '\n\n')
cat(c2_declarations_cleaned_weak2)
```

```{r}
# fit_mlr_weak2 <- cfa(model = c2_declarations_cleaned_weak2, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_mlr_weak2, './cfa_models/fit_mlr_weak2')
fit_mlr_weak2 <- readRDS('./cfa_models/fit_mlr_weak2')
```

Метрики качества получившейся модели:
```{r}
fitMeasures(fit_mlr_weak, c('cfi', 'rmsea', 'srmr'))
```


## метрики с2-моделей

Сводная таблица метрик качества этих моделей выглядит следующим образом:

```{r}
c2_models_metrics <- data.table(
  model = c(
    'исходная модель', 
    'модель без незначимых вопросов', 
    'модель с кросс-нагрузками', 
    'модель без кросс-нагрузок',
    'модель с высокими EFA-нагрузками',
    'модель без низких EFA-нагрузок по отдельным шкалам')
)
```

```{r}
metrics_names <- c('cfi', 'rmsea', 'srmr')
c2_models_metrics[model == 'исходная модель', (metrics_names) := as.list(round(fitMeasures(fit_mlr, metrics_names), 3))]
c2_models_metrics[model == 'модель без незначимых вопросов', (metrics_names) := as.list(round(fitMeasures(fit_mlr_cleaned, metrics_names), 3))]
c2_models_metrics[model == 'модель с кросс-нагрузками', (metrics_names) := as.list(round(fitMeasures(fit_mlr_mod, metrics_names), 3))]
c2_models_metrics[model == 'модель без кросс-нагрузок', (metrics_names) := as.list(round(fitMeasures(fit_mlr_noloads_15, metrics_names), 3))]
c2_models_metrics[model == 'модель с высокими EFA-нагрузками', (metrics_names) := as.list(round(fitMeasures(fit_mlr_weak, metrics_names), 3))]
c2_models_metrics[model == 'модель без низких EFA-нагрузок по отдельным шкалам', (metrics_names) := as.list(round(fitMeasures(fit_mlr_weak2, metrics_names), 3))]

kableExtra::kable(c2_models_metrics)  %>%
  kable_styling(font_size = 12)
```

Удаление незначимых вопросов немного улучшает качество структуры модели, но оно все еще недостаточно. Остальные попытки улучшить качество модели не дали хороших эффектов, даже наоборот. 

Дальнейшие улучшения текущей можели на восемь компетенций мне кажутся бессмысленными. Тем не менее, можно попробовать оценить качество модели по 30 субшкалам (то есть по субшкалам наших восьми компетенций), что я и сделал ниже. Либо в целом попробовать пересобрать факторную модель в эксплораторном факторном анализе (в следующем разделе).


## модель с3-шкал

В качестве альтернативного решения я попробовал оценить структуру опросника на уровне дробных с3-шкал. Может быть, осмысленно спуститься на уровень ниже, от восьми шкал компетенций на уровень из субшкал (исходно в модели их 42, в опроснике представлено 30)

Однако модель на 30 факторов не сходится, даже если предположить скоррелированность латентных факторов.

```{r}
c3_names <- competencies_info[, sort(unique(c3))]

c3_declarations <- lapply(c3_names, function(x) competencies_info[c3 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c3_declarations <- paste(c3_declarations, collapse = '\n\n')
# cat(c3_declarations)
```


```{r}
# fit_c3_mlr <- cfa(model = c3_declarations, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_c3_mlr, './cfa_models/fit_c3_mlr')
fit_c3_mlr <- readRDS('./cfa_models/fit_c3_mlr')
```


Если рассматривать каждую из 30 шкал отдельно, то видно, что есть несколько шкал, по которым даже маленькая модель сойтись не может. 

```{r, include=FALSE}
# c3_models <- lapply(c3_names, function(x) {
#   tmp_model <- competencies_info[c3 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))]
#   tmp_model_fit <- cfa(model = tmp_model, data = competencies_data, estimator = 'MLR')
#   return(tmp_model_fit)
# })
# 
# saveRDS(c3_models, './cfa_models/c3_models')
c3_models <- readRDS('./cfa_models/c3_models')
```



```{r}
c3_models_tbl <- lapply(1:length(c3_names), function(x) {
  result <- tryCatch(
      # as.data.table(as.list(c('c3' = c3_names[x], fitMeasures(c3_models[[x]], c("cfi", "rmsea", "srmr", "chisq", "df"))))),
      as.data.table(as.list(c('c3' = c3_names[x], round(fitMeasures(c3_models[[x]], c("cfi", "rmsea", "srmr")), 3)))),
    error = function(e) data.table(c3 = c3_names[x], cfi = NA, rmsea = NA, srmr = NA)
  )
  # print(result)
  result
})
c3_models_tbl <- rbindlist(c3_models_tbl)
c3_models_tbl <- merge(
  competencies_info[, list(c2 = unique(c2), n_questions = .N), keyby = c3], 
  c3_models_tbl, 
  by = 'c3', all.x = TRUE
)
c3_models_tbl[, cfi := cell_spec(cfi, color = ifelse(cfi >= 0.9 | is.na(cfi), "black", "red"))]
c3_models_tbl[, rmsea := cell_spec(rmsea, color = ifelse(rmsea < 0.08 | is.na(rmsea), "black", "red"))]
c3_models_tbl[, srmr := cell_spec(srmr, color = ifelse(srmr < 0.08 | is.na(srmr), "black", "red"))]

kableExtra::kable(c3_models_tbl, escape = FALSE, format = "html") %>%
  kable_styling(font_size = 12)
```


## модель с1-шкал

Еще одно альтернативное решение --- подняться на уровень макро-компетенций (всего пять).

Декларация модели:
```{r}
c1_names <- competencies_info[, sort(unique(c1))]

c1_declarations <- lapply(c1_names, function(x) competencies_info[c1 == x & question_ru %in% c2_questions, paste(glue('c{x} =~'), paste(q, collapse = ' + '))])
c1_declarations <- paste(c1_declarations, collapse = '\n\n')
cat(c3_declarations)
```

```{r}
# fit_c1_mlr <- cfa(model = c1_declarations, data = competencies_data, estimator = 'MLR')
# saveRDS(fit_c1_mlr, './cfa_models/fit_c1_mlr')
fit_c1_mlr <- readRDS('./cfa_models/fit_c1_mlr')
```

Метрики качества получившейся модели:
```{r}
fitMeasures(fit_c1_mlr, c('cfi', 'rmsea', 'srmr'))
```

